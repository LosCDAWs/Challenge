{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge\n",
    "\n",
    "\n",
    "# 1. Knowledge domain\n",
    "# 2. Exploratory analysis\n",
    "# 3. Machine learning model\n",
    "## 3.1. First approach\n",
    "## 3.2. Second approach\n",
    "# 4. Result of the competition\n",
    "# References\n",
    "\n",
    "------------------\n",
    "\n",
    "En este notebook:\n",
    "\n",
    "## 3.2. Second approach\n",
    "### Knn\n",
    "### Decision Tree\n",
    "### Gaussian NB\n",
    "### Random Forest\n",
    "# 4. Result of the competition\n",
    "# References\n",
    "\n",
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Second approach\n",
    "\n",
    "### Cargo Valores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pandas import Series, DataFrame\n",
    "\n",
    "data = pd.read_csv('data/train.csv', sep=\",\", encoding = \"ISO-8859-1\")\n",
    "df = data.copy()\n",
    "\n",
    "data_test_nolabel = pd.read_csv('data/test_nolabel.csv', sep=\",\", encoding = \"ISO-8859-1\")\n",
    "df_test_nolabel = data_test_nolabel.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_nolabel = pd.read_csv('data/test_nolabel.csv', sep=\",\", encoding = \"ISO-8859-1\")\n",
    "df_test_nolabel = data_test_nolabel.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cambios en Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DisbursementDate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop ChgOffDate column\n",
    "#df.drop(['ChgOffDate'], axis=1, inplace=True)\n",
    "df.drop(['State'], axis=1, inplace=True)\n",
    "df.drop(['BalanceGross'], axis=1, inplace=True)\n",
    "df.drop(['id'], axis=1, inplace=True)\n",
    "\n",
    "# drop ChgOffDate column\n",
    "#df_test_nolabel.drop(['ChgOffDate'], axis=1, inplace=True)\n",
    "df_test_nolabel.drop(['State'], axis=1, inplace=True)\n",
    "df_test_nolabel.drop(['BalanceGross'], axis=1, inplace=True)\n",
    "#df_test_nolabel.drop(['id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change the format into datetime\n",
    "df['DisbursementDate'] = pd.to_datetime(data['DisbursementDate'])\n",
    "\n",
    "#Change the format into datetime\n",
    "df_test_nolabel['DisbursementDate'] = pd.to_datetime(data['DisbursementDate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['DisbursementDay'] = df['DisbursementDate'].apply(lambda x: x.day)\n",
    "\n",
    "df_test_nolabel['DisbursementDay'] = df_test_nolabel['DisbursementDate'].apply(lambda x: x.day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['DisbursementMonth'] = df['DisbursementDate'].apply(lambda x: x.month)\n",
    "df_test_nolabel['DisbursementMonth'] = df_test_nolabel['DisbursementDate'].apply(lambda x: x.month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['DisbursementYear'] = df['DisbursementDate'].apply(lambda x: x.year)\n",
    "df_test_nolabel['DisbursementYear'] = df_test_nolabel['DisbursementDate'].apply(lambda x: x.year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now delete the 'DisbursementDate' column.\n",
    "df.drop(['DisbursementDate'], axis=1, inplace=True)\n",
    "# We can now delete the 'DisbursementDate' column.\n",
    "df_test_nolabel.drop(['DisbursementDate'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ApprovalDate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change the format into datetime\n",
    "df['ApprovalDate'] = pd.to_datetime(data['ApprovalDate'])\n",
    "#Change the format into datetime\n",
    "df_test_nolabel['ApprovalDate'] = pd.to_datetime(data['ApprovalDate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ApprovalDay'] = df['ApprovalDate'].apply(lambda x: x.day)\n",
    "df['ApprovalMonth'] = df['ApprovalDate'].apply(lambda x: x.month)\n",
    "df['ApprovalYear'] = df['ApprovalDate'].apply(lambda x: x.year)\n",
    "\n",
    "df_test_nolabel['ApprovalDay'] = df_test_nolabel['ApprovalDate'].apply(lambda x: x.day)\n",
    "df_test_nolabel['ApprovalMonth'] = df_test_nolabel['ApprovalDate'].apply(lambda x: x.month)\n",
    "df_test_nolabel['ApprovalYear'] = df_test_nolabel['ApprovalDate'].apply(lambda x: x.year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now delete the 'ApprovalDate' column.\n",
    "df.drop(['ApprovalDate'], axis=1, inplace=True)\n",
    "# We can now delete the 'ApprovalDate' column.\n",
    "df_test_nolabel.drop(['ApprovalDate'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DisbursementGross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_currency(x):\n",
    "   \n",
    "    if isinstance(x, str):\n",
    "        return(x.replace('$', '').replace(',', ''))\n",
    "    return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['DisbursementGross'] = df['DisbursementGross'].apply(clean_currency).astype('float')\n",
    "df_test_nolabel['DisbursementGross'] = df_test_nolabel['DisbursementGross'].apply(clean_currency).astype('float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChgOffDate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ChgOffDate'] = df['ChgOffDate'].fillna(1)\n",
    "\n",
    "for i in range(len(df)):\n",
    "    if df['ChgOffDate'][i] != 1 :\n",
    "        df['ChgOffDate'][i] = 0   \n",
    "\n",
    "df['ChgOffDate'] = df['ChgOffDate'].astype(int)\n",
    "\n",
    "df_test_nolabel['ChgOffDate'] = df_test_nolabel['ChgOffDate'].fillna(1)\n",
    "\n",
    "for i in range(len(df_test_nolabel)):\n",
    "    if df_test_nolabel['ChgOffDate'][i] != 1 :\n",
    "        df_test_nolabel['ChgOffDate'][i] = 0   \n",
    "        \n",
    "df_test_nolabel['ChgOffDate'] = df_test_nolabel['ChgOffDate'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = []\n",
    "\n",
    "lista_1 = dict()\n",
    "for i in range (0, 49320): \n",
    "    name.append(df['Name'][i])\n",
    "    \n",
    "for j in range (0, 49320):\n",
    "        \n",
    "    if df['Name'][j] not in lista_1: \n",
    "        c = name[j]\n",
    "        df['Name'][j] = j\n",
    "        lista_1[c] = j\n",
    "        print(c)\n",
    "        print(df['Name'][j])\n",
    "        print(lista_1)\n",
    "        \n",
    "    else :\n",
    "        df['Name'][j] = lista_1[df['Name'][j]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = []\n",
    "\n",
    "lista_1 = dict()\n",
    "for i in range (0, 7994): \n",
    "    name.append(df_test_nolabel['Name'][i])\n",
    "    \n",
    "for j in range (0, 7994):\n",
    "        \n",
    "    if df_test_nolabel['Name'][j] not in lista_1: \n",
    "        c = name[j]\n",
    "        df_test_nolabel['Name'][j] = j\n",
    "        lista_1[c] = j\n",
    "        print(c)\n",
    "        print(df_test_nolabel['Name'][j])\n",
    "        print(lista_1)\n",
    "        \n",
    "    else :\n",
    "        df_test_nolabel['Name'][j] = lista_1[df_test_nolabel['Name'][j]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_bank = []\n",
    "lista = dict()\n",
    "\n",
    "for i in range (0, 49320):\n",
    "    name_bank.append(df['Bank'][i])\n",
    "    \n",
    "for j in range (0, 49320):\n",
    "    \n",
    "    if df['Bank'][j] not in lista:\n",
    "        c = name_bank[j]\n",
    "        df['Bank'][j] = j\n",
    "        lista[c] = j\n",
    "        print(c)\n",
    "        print(df['Bank'][j])\n",
    "        print(lista)\n",
    "        \n",
    "    else :\n",
    "        df['Bank'][j] = lista[df['Bank'][j]]\n",
    "        \n",
    "name_bank = []\n",
    "lista = dict()\n",
    "\n",
    "for i in range (0, 7994):\n",
    "    name_bank.append(df_test_nolabel['Bank'][i])\n",
    "    \n",
    "for j in range (0, 7994):\n",
    "    \n",
    "    if df_test_nolabel['Bank'][j] not in lista:\n",
    "        c = name_bank[j]\n",
    "        df_test_nolabel['Bank'][j] = j\n",
    "        lista[c] = j\n",
    "        print(c)\n",
    "        print(df_test_nolabel['Bank'][j])\n",
    "        print(lista)\n",
    "        \n",
    "    else :\n",
    "        df_test_nolabel['Bank'][j] = lista[df_test_nolabel['Bank'][j]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "City = []\n",
    "lista_2 = dict()\n",
    "\n",
    "for i in range (0, 49320):\n",
    "    City.append(df['City'][i])\n",
    "    \n",
    "for j in range (0, 49320):\n",
    "    \n",
    "    if df['City'][j] not in lista_2:\n",
    "        c = City[j]\n",
    "        df['City'][j] = j\n",
    "        lista_2[c] = j\n",
    "        print(c)\n",
    "        print(df['City'][j])\n",
    "        print(lista_2)\n",
    "        \n",
    "    else :\n",
    "        df['City'][j] = lista_2[df['City'][j]]\n",
    "        \n",
    "\n",
    "City = []\n",
    "lista_2 = dict()\n",
    "\n",
    "for i in range (0, 7994):\n",
    "    City.append(df_test_nolabel['City'][i])\n",
    "    \n",
    "for j in range (0, 7994):\n",
    "    \n",
    "    if df_test_nolabel['City'][j] not in lista_2:\n",
    "        c = City[j]\n",
    "        df_test_nolabel['City'][j] = j\n",
    "        lista_2[c] = j\n",
    "        print(c)\n",
    "        print(df_test_nolabel['City'][j])\n",
    "        print(lista_2)\n",
    "        \n",
    "    else :\n",
    "        df_test_nolabel['City'][j] = lista_2[df_test_nolabel['City'][j]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BankState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BankState = []\n",
    "lista_4 = dict()\n",
    "\n",
    "for i in range (0, 49320):\n",
    "    BankState.append(df['BankState'][i])\n",
    "    \n",
    "for j in range (0, 49320):\n",
    "\n",
    "    if df['BankState'][j] not in lista_4:\n",
    "        c = BankState[j]\n",
    "        df['BankState'][j] = j\n",
    "        lista_4[c] = j\n",
    "        print(c)\n",
    "        print(df['BankState'][j])\n",
    "        print(lista_4)\n",
    "        \n",
    "    else :\n",
    "        df['BankState'][j] = lista_4[df['BankState'][j]]\n",
    "        \n",
    "BankState = []\n",
    "lista_4 = dict()\n",
    "\n",
    "for i in range (0, 7994):\n",
    "    BankState.append(df_test_nolabel['BankState'][i])\n",
    "    \n",
    "for j in range (0, 7994):\n",
    "\n",
    "    if df_test_nolabel['BankState'][j] not in lista_4:\n",
    "        c = BankState[j]\n",
    "        df_test_nolabel['BankState'][j] = j\n",
    "        lista_4[c] = j\n",
    "        print(c)\n",
    "        print(df_test_nolabel['BankState'][j])\n",
    "        print(lista_4)\n",
    "        \n",
    "    else :\n",
    "        df_test_nolabel['BankState'][j] = lista_4[df_test_nolabel['BankState'][j]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ApprovalFY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ApprovalFY = []\n",
    "lista_6 = dict()\n",
    "\n",
    "for i in range (0, 49320):\n",
    "    ApprovalFY.append(df['ApprovalFY'][i])\n",
    "    \n",
    "for j in range (0, 49320):\n",
    "    \n",
    "    if df['ApprovalFY'][j] not in lista_6:\n",
    "        c = ApprovalFY[j]\n",
    "        df['ApprovalFY'][j] = j\n",
    "        lista_6[c] = j\n",
    "        print(c)\n",
    "        print(df['ApprovalFY'][j])\n",
    "        print(lista_6)\n",
    "        \n",
    "    else :\n",
    "        df['ApprovalFY'][j] = lista_6[df['ApprovalFY'][j]]\n",
    "        \n",
    "ApprovalFY = []\n",
    "lista_6 = dict()\n",
    "\n",
    "for i in range (0, 7994):\n",
    "    ApprovalFY.append(df_test_nolabel['ApprovalFY'][i])\n",
    "    \n",
    "for j in range (0, 7994):\n",
    "    \n",
    "    if df_test_nolabel['ApprovalFY'][j] not in lista_6:\n",
    "        c = ApprovalFY[j]\n",
    "        df_test_nolabel['ApprovalFY'][j] = j\n",
    "        lista_6[c] = j\n",
    "        print(c)\n",
    "        print(df_test_nolabel['ApprovalFY'][j])\n",
    "        print(lista_6)\n",
    "        \n",
    "    else :\n",
    "        df_test_nolabel['ApprovalFY'][j] = lista_6[df_test_nolabel['ApprovalFY'][j]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RevLineCr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RevLineCr = []\n",
    "lista_7 = dict()\n",
    "\n",
    "for i in range (0, 49320):\n",
    "    RevLineCr.append(df['RevLineCr'][i])\n",
    "    \n",
    "for j in range (0, 49320):\n",
    "        \n",
    "    if df['RevLineCr'][j] not in lista_7:\n",
    "        c = RevLineCr[j]\n",
    "        df['RevLineCr'][j] = j\n",
    "        lista_7[c] = j\n",
    "        print(c)\n",
    "        print(df['RevLineCr'][j])\n",
    "        print(lista_7)\n",
    "        \n",
    "    else :\n",
    "        df['RevLineCr'][j] = lista_7[df['RevLineCr'][j]]\n",
    "        \n",
    "RevLineCr = []\n",
    "lista_7 = dict()\n",
    "\n",
    "for i in range (0, 7994):\n",
    "    RevLineCr.append(df_test_nolabel['RevLineCr'][i])\n",
    "    \n",
    "for j in range (0, 7994):\n",
    "        \n",
    "    if df_test_nolabel['RevLineCr'][j] not in lista_7:\n",
    "        c = RevLineCr[j]\n",
    "        df_test_nolabel['RevLineCr'][j] = j\n",
    "        lista_7[c] = j\n",
    "        print(c)\n",
    "        print(df_test_nolabel['RevLineCr'][j])\n",
    "        print(lista_7)\n",
    "        \n",
    "    else :\n",
    "        df_test_nolabel['RevLineCr'][j] = lista_7[df_test_nolabel['RevLineCr'][j]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LowDoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LowDoc = []\n",
    "lista_8 = dict()\n",
    "\n",
    "for i in range (0, 49320):\n",
    "    LowDoc.append(df['LowDoc'][i])\n",
    "    \n",
    "for j in range (0, 49320):\n",
    "        \n",
    "    if df['LowDoc'][j] not in lista_8:\n",
    "        c = LowDoc[j]\n",
    "        df['LowDoc'][j] = j\n",
    "        lista_8[c] = j\n",
    "        print(c)\n",
    "        print(df['LowDoc'][j])\n",
    "        print(lista_8)\n",
    "        \n",
    "    else :\n",
    "        df['LowDoc'][j] = lista_8[df['LowDoc'][j]]\n",
    "        \n",
    "LowDoc = []\n",
    "lista_8 = dict()\n",
    "\n",
    "for i in range (0, 7994):\n",
    "    LowDoc.append(df_test_nolabel['LowDoc'][i])\n",
    "    \n",
    "for j in range (0, 7994):\n",
    "        \n",
    "    if df_test_nolabel['LowDoc'][j] not in lista_8:\n",
    "        c = LowDoc[j]\n",
    "        df_test_nolabel['LowDoc'][j] = j\n",
    "        lista_8[c] = j\n",
    "        print(c)\n",
    "        print(df_test_nolabel['LowDoc'][j])\n",
    "        print(lista_8)\n",
    "        \n",
    "    else :\n",
    "        df_test_nolabel['LowDoc'][j] = lista_8[df_test_nolabel['LowDoc'][j]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Name'] = df['Name'].astype(float)\n",
    "df['City'] = df['City'].astype(float)\n",
    "df['Bank'] = df['Bank'].astype(float)\n",
    "df['BankState'] = df['BankState'].astype(float)\n",
    "df['ApprovalFY'] = df['ApprovalFY'].astype(float)\n",
    "df['RevLineCr'] = df['RevLineCr'].astype(float)\n",
    "df['LowDoc'] = df['LowDoc'].astype(float)\n",
    "\n",
    "df_test_nolabel['Name'] = df_test_nolabel['Name'].astype(float)\n",
    "df_test_nolabel['City'] = df_test_nolabel['City'].astype(float)\n",
    "df_test_nolabel['Bank'] = df_test_nolabel['Bank'].astype(float)\n",
    "df_test_nolabel['BankState'] = df_test_nolabel['BankState'].astype(float)\n",
    "df_test_nolabel['ApprovalFY'] = df_test_nolabel['ApprovalFY'].astype(float)\n",
    "df_test_nolabel['RevLineCr'] = df_test_nolabel['RevLineCr'].astype(float)\n",
    "df_test_nolabel['LowDoc'] = df_test_nolabel['LowDoc'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_nolabel.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_test_nolabel.isna().sum()/df_test_nolabel.shape[0]).sort_values(ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropeo de NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the Nan of the variables which were of type float/int since the beginning\n",
    "# NO HACER EN EL TEST\n",
    "df.dropna(axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_nolabel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexNames = df[ df['LowDoc'] == 32 ].index\n",
    "# Delete these row indexes from dataFrame\n",
    "df.drop(indexNames , inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexNames = df[ df['BankState'] == 946 ].index\n",
    "# Delete these row indexes from dataFrame\n",
    "df.drop(indexNames , inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexNames = df[ df['RevLineCr'] == 145].index\n",
    "# Delete these row indexes from dataFrame\n",
    "df.drop(indexNames , inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df\n",
    "\n",
    "def corr_heatmap(train):\n",
    "    correlations = train.corr()\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(20,20))\n",
    "    sns.heatmap(correlations, vmax=0.1, center=0, fmt='.2f',cmap=\"PiYG\",\n",
    "                square=True, linewidths=.5, annot=True, cbar_kws={\"shrink\": 0.9})\n",
    "    plt.show();\n",
    "    \n",
    "corr_heatmap(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Test Split\n",
    "#Use train_test_split to split your data into a training set and a testing set\n",
    "from sklearn.model_selection import train_test_split\n",
    "features = ['ChgOffDate', 'UrbanRural', 'ApprovalYear']\n",
    "X=df[features]\n",
    "y=df['Accept']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using KNN\n",
    "#Import KNeighborsClassifier from scikit learn.\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a KNN model instance with n_neighbors=\n",
    "knn = KNeighborsClassifier(n_neighbors = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=4)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fit this KNN model to the training data.\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predictions and Evaluations\n",
    "#Let's evaluate our KNN model!\n",
    "#Use the predict method to predict values using your KNN model and X_test.\n",
    "\n",
    "pred = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a confusion matrix and classification report.\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1896    6]\n",
      " [  38 7770]]\n"
     ]
    }
   ],
   "source": [
    "#Calculate the confusion matrix, which will help us to know the data that has \n",
    "#been correctly predicted (main diagonal = 6730), and the wrong data (secondary diagonal = 1916)\n",
    "print(confusion_matrix(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      1902\n",
      "           1       1.00      1.00      1.00      7808\n",
      "\n",
      "    accuracy                           1.00      9710\n",
      "   macro avg       0.99      1.00      0.99      9710\n",
      "weighted avg       1.00      1.00      1.00      9710\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Calculate model precision\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score:  0.9931508334198516\n",
      "Best params:  {'n_neighbors': 2}\n",
      "0.986 (+/-0.016) for {'n_neighbors': 1}\n",
      "0.993 (+/-0.002) for {'n_neighbors': 2}\n",
      "0.993 (+/-0.002) for {'n_neighbors': 3}\n",
      "0.993 (+/-0.002) for {'n_neighbors': 4}\n",
      "0.993 (+/-0.002) for {'n_neighbors': 5}\n",
      "0.993 (+/-0.002) for {'n_neighbors': 6}\n",
      "0.993 (+/-0.002) for {'n_neighbors': 7}\n",
      "0.993 (+/-0.002) for {'n_neighbors': 8}\n",
      "0.993 (+/-0.002) for {'n_neighbors': 9}\n"
     ]
    }
   ],
   "source": [
    "#We use this code to find a best K number to our algorithm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'n_neighbors': np.arange(1, 10,1)} \n",
    "gs = GridSearchCV(KNeighborsClassifier(), param_grid)\n",
    "gs.fit(X_train, y_train)\n",
    "print(\"Best score: \", gs.best_score_)\n",
    "print(\"Best params: \", gs.best_params_)\n",
    "for i, max_depth in enumerate(gs.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\" % (gs.cv_results_['mean_test_score'][i],\n",
    "                                        gs.cv_results_['std_test_score'][i] * 2,\n",
    "                                        max_depth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1896    6]\n",
      " [  38 7770]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      1902\n",
      "           1       1.00      1.00      1.00      7808\n",
      "\n",
      "    accuracy                           1.00      9710\n",
      "   macro avg       0.99      1.00      0.99      9710\n",
      "weighted avg       1.00      1.00      1.00      9710\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#kNN para 9 vecinos\n",
    "knn = KNeighborsClassifier(n_neighbors =4)\n",
    "knn.fit(X_train, y_train)\n",
    "pred = knn.predict(X_test)\n",
    "print(confusion_matrix(y_test, pred))\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN: Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_test = df_test_nolabel['id']\n",
    "\n",
    "features = ['ChgOffDate', 'UrbanRural', 'ApprovalYear']\n",
    "X=df_test_nolabel[features]\n",
    "y_pred=knn.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-98-c3a384c4651c>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  results[\"AcceptFloat\"] = y_pred\n",
      "<ipython-input-98-c3a384c4651c>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  results[\"Accept\"] = results[\"AcceptFloat\"].values.astype(int)\n",
      "C:\\Users\\Alejandro\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:4163: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7994"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = df_test_nolabel[[\"id\"]]\n",
    "results[\"AcceptFloat\"] = y_pred\n",
    "results[\"Accept\"] = results[\"AcceptFloat\"].values.astype(int)\n",
    "results.drop(['AcceptFloat'], axis=1, inplace=True)\n",
    "results[\"id\"].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(\"resultsKNN_good.csv\", index_label=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and test spliting\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "\n",
    "# Si se incluyen features con nan no funciona el árbol\n",
    "features = ['ChgOffDate', 'UrbanRural', 'ApprovalYear']\n",
    "\n",
    "# Transform dataframe in numpy arrays\n",
    "X = df[features].values\n",
    "y = df['Accept'].values\n",
    "\n",
    "# Test set will be the 25% taken randomly\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=33)\n",
    "\n",
    "# Preprocess: normalize\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'max_depth': np.arange(1, 20)} \n",
    "\n",
    "gs = GridSearchCV(DecisionTreeClassifier(), param_grid)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "# summarize the results of the grid search\n",
    "print(\"Best score: \", gs.best_score_)\n",
    "print(\"Best params: \", gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We print the score for each value of max_depth\n",
    "for i, max_depth in enumerate(gs.cv_results_['params']):\n",
    "    print(\"%0.4f (+/-%0.04f) for %r\" % (gs.cv_results_['mean_test_score'][i],\n",
    "                                        gs.cv_results_['std_test_score'][i] * 2,\n",
    "                                        max_depth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score, KFold, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "model = tree.DecisionTreeClassifier(criterion='gini',\n",
    "                                            splitter=\"best\",\n",
    "                                            min_samples_split=3,\n",
    "                                            min_samples_leaf=3,\n",
    "                                            max_depth = 3,\n",
    "                                            class_weight= {0:0.96,1:0.96} )\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "predicted = model.predict(X_test)\n",
    "expected = y_test\n",
    "\n",
    "# Accuracy\n",
    "metrics.accuracy_score(expected, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=20, shuffle=False)\n",
    "scores = cross_val_score(model, X, y, cv=cv)\n",
    "\n",
    "print(\"Scores in every iteration\", scores)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = model.predict(X_train)\n",
    "print(\"Accuracy in training\", metrics.accuracy_score(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_test = df_test_nolabel['id']\n",
    "\n",
    "features = ['ChgOffDate', 'UrbanRural', 'ApprovalYear']\n",
    "X=df_test_nolabel[features]\n",
    "y_pred=model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = df_test_nolabel[[\"id\"]]\n",
    "results[\"AcceptFloat\"] = y_pred\n",
    "results[\"Accept\"] = results[\"AcceptFloat\"].values.astype(int)\n",
    "results.drop(['AcceptFloat'], axis=1, inplace=True)\n",
    "results[\"id\"].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(\"resultsDT_good.csv\", index_label=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score,f1_score\n",
    "\n",
    "\n",
    "tree_clf = DecisionTreeClassifier()\n",
    "tree_clf.fit(X_train,y_train)\n",
    "y_pred = tree_clf.predict(X_train)\n",
    "\n",
    "print(\"Training Data Set Accuracy: \", accuracy_score(y_train,y_pred))\n",
    "print(\"Training Data F1 Score \", f1_score(y_train,y_pred))\n",
    "\n",
    "print(\"Validation Mean F1 Score: \",cross_val_score(tree_clf,X_train,y_train,cv=5,scoring='f1_macro').mean())\n",
    "print(\"Validation Mean Accuracy: \",cross_val_score(tree_clf,X_train,y_train,cv=5,scoring='accuracy').mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OverFitting Problem : We can see from above metrics that Training Accuracy > Test Accuracy with default settings of Decision Tree classifier. Hence, model is overfit. We will try some Hyper-parameter tuning and see if it helps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's try tuning 'Max_Depth' of tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_accuracy = []\n",
    "val_accuracy = []\n",
    "training_f1 = []\n",
    "val_f1 = []\n",
    "tree_depths = []\n",
    "\n",
    "for depth in range(1,20):\n",
    "    tree_clf = DecisionTreeClassifier(max_depth=depth)\n",
    "    tree_clf.fit(X_train,y_train)\n",
    "    y_training_pred = tree_clf.predict(X_train)\n",
    "\n",
    "    training_acc = accuracy_score(y_train,y_training_pred)\n",
    "    train_f1 = f1_score(y_train,y_training_pred)\n",
    "    val_mean_f1 = cross_val_score(tree_clf,X_train,y_train,cv=5,scoring='f1_macro').mean()\n",
    "    val_mean_accuracy = cross_val_score(tree_clf,X_train,y_train,cv=5,scoring='accuracy').mean()\n",
    "    \n",
    "    training_accuracy.append(training_acc)\n",
    "    val_accuracy.append(val_mean_accuracy)\n",
    "    training_f1.append(train_f1)\n",
    "    val_f1.append(val_mean_f1)\n",
    "    tree_depths.append(depth)\n",
    "    \n",
    "\n",
    "Tuning_Max_depth = {\"Training Accuracy\": training_accuracy, \"Validation Accuracy\": val_accuracy, \"Training F1\": training_f1, \"Validation F1\":val_f1, \"Max_Depth\": tree_depths }\n",
    "Tuning_Max_depth_df = pd.DataFrame.from_dict(Tuning_Max_depth)\n",
    "\n",
    "plot_df = Tuning_Max_depth_df.melt('Max_Depth',var_name='Metrics',value_name=\"Values\")\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(15,5))\n",
    "sns.pointplot(x=\"Max_Depth\", y=\"Values\",hue=\"Metrics\", data=plot_df,ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above graph, we can conclude that keeping 'Max_Depth' = 3 will yield optimum Test accuracy and F1 score Optimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visulazing Decision Tree with Max Depth = 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "tree_clf = tree.DecisionTreeClassifier(max_depth = 3)\n",
    "tree_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "tree.plot_tree(tree_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GaussianNB: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Test Split\n",
    "#Use train_test_split to split your data into a training set and a testing set\n",
    "from sklearn.model_selection import train_test_split\n",
    "features= ['ChgOffDate', 'UrbanRural', 'ApprovalYear']\n",
    "X=df[features]\n",
    "y=df['Accept']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Value: [1 0 1 ... 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "#Import Gaussian Naive Bayes model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "#Create a Gaussian Classifier\n",
    "model = GaussianNB()\n",
    "\n",
    "# Train the model using the training sets\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "#Predict Output\n",
    "predicted= model.predict(X_test) # 0:Overcast, 2:Mild\n",
    "print (\"Predicted Value:\", predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      1902\n",
      "           1       1.00      1.00      1.00      7808\n",
      "\n",
      "    accuracy                           1.00      9710\n",
      "   macro avg       0.99      1.00      0.99      9710\n",
      "weighted avg       1.00      1.00      1.00      9710\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Calculate model precision\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GaussianNB: Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_test = df_test_nolabel['id']\n",
    "\n",
    "features = ['ChgOffDate', 'UrbanRural', 'ApprovalYear']\n",
    "X=df_test_nolabel[features]\n",
    "y_pred=model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = df_test_nolabel[[\"id\"]]\n",
    "results[\"AcceptFloat\"] = y_pred\n",
    "results[\"Accept\"] = results[\"AcceptFloat\"].values.astype(int)\n",
    "results.drop(['AcceptFloat'], axis=1, inplace=True)\n",
    "results[\"id\"].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(\"resultsGaussianNB_good.csv\", index_label=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#On divise notre Dataset en Train set et en Test set\n",
    "\n",
    "train_set, test_set = train_test_split(df, test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['Accept', 'id'], axis=1)\n",
    "y = df['Accept']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(df):\n",
    "    \n",
    "    X = df.drop(['Accept', 'id'], axis=1)\n",
    "    y = df['Accept']\n",
    "    \n",
    "    print(X.shape, y.shape)\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = preprocessing(train_set) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = preprocessing(test_set) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Result of the competition\n",
    "### A. Introducción\n",
    "### B. Modelo Geométrico: KNN\n",
    "### C. Modelo Decision Tree\n",
    "### D. Modelo Gaussian NB\n",
    "-----------------------------------------\n",
    "### A. Introducción\n",
    "Una vez disponga de un modelo, deberá usar la plataforma Kaggle para probar su modelo con datos, que combinan datos nuevos con los datos existentes. Puede realizar tantos intentos como desee. \n",
    "Razone los resultados en el cuaderno.\n",
    "### B. Modelo Geométrico: KNN\n",
    "Knn para 9 vecinos con todas las features preprocesadas\n",
    " \n",
    "      precision    recall  f1-score   support\n",
    "\n",
    "           0       0.28      0.28      0.28      2817\n",
    "           1       0.83      0.83      0.83     11748\n",
    "\n",
    "    accuracy                           0.72     14565\n",
    "   macro avg       0.55      0.55      0.55     14565\n",
    "weighted avg       0.72      0.72      0.72     14565\n",
    "\n",
    "Best score:  0.7972748684351014\n",
    "Best params:  {'n_neighbors': 9}\n",
    "\n",
    "  Knn para 9 vecinos con 4 features preprocesadas\n",
    "  \n",
    "       precision    recall  f1-score   support\n",
    "\n",
    "           0       0.36      0.08      0.14      2817\n",
    "           1       0.81      0.96      0.88     11748\n",
    "\n",
    "    accuracy                           0.79     14565\n",
    "   macro avg       0.59      0.52      0.51     14565\n",
    "weighted avg       0.73      0.79      0.74     14565\n",
    "\n",
    "Best score:  0.7972748684351014\n",
    "Best params:  {'n_neighbors': 9}\n",
    "\n",
    "Knn con 4 features: 0.62095\n",
    "\n",
    "\n",
    "Con 4 vecinos\n",
    "\n",
    "                 precision    recall  f1-score   support\n",
    "\n",
    "           0       0.98      1.00      0.99      1902\n",
    "           1       1.00      1.00      1.00      7808\n",
    "\n",
    "    accuracy                           1.00      9710\n",
    "   macro avg       0.99      1.00      0.99      9710\n",
    "weighted avg       1.00      1.00      1.00      9710\n",
    "\n",
    "Con 9 vecinos\n",
    "\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.98      1.00      0.99      1902\n",
    "           1       1.00      1.00      1.00      7808\n",
    "\n",
    "    accuracy                           1.00      9710\n",
    "   macro avg       0.99      1.00      0.99      9710\n",
    "weighted avg       1.00      1.00      1.00      9710\n",
    "\n",
    "\n",
    "### C. Modelo Decision Tree\n",
    "\n",
    "Decison tree con todas las variables (bueno, las que no tienen nan) preprocesadas \n",
    "\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.71      0.22      0.34      6892\n",
    "           1       0.84      0.98      0.91     29518\n",
    "\n",
    "    accuracy                           0.84     36410\n",
    "   macro avg       0.78      0.60      0.62     36410\n",
    "weighted avg       0.82      0.84      0.80     36410\n",
    "\n",
    "Accuracy in training 0.8353474320241692\n",
    "\n",
    "Best score:  0.8292776709695138\n",
    "Best params:  {'max_depth': 6}\n",
    "\n",
    "0.66588 en kaagle \n",
    "\n",
    "Decison tree con 4 variables preprocesadas\n",
    "\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.71      0.20      0.31      6892\n",
    "           1       0.84      0.98      0.90     29518\n",
    "\n",
    "    accuracy                           0.83     36410\n",
    "   macro avg       0.77      0.59      0.61     36410\n",
    "weighted avg       0.81      0.83      0.79     36410\n",
    "\n",
    "0.8155251893315772\n",
    "\n",
    "\n",
    "Tienen mejor valor en el 0 que el knn\n",
    "\n",
    "                precision    recall  f1-score   support\n",
    "\n",
    "           0       0.97      1.00      0.98      8717\n",
    "           1       1.00      0.99      1.00     37402\n",
    "\n",
    "    accuracy                           0.99     46119\n",
    "   macro avg       0.98      0.99      0.99     46119\n",
    "weighted avg       0.99      0.99      0.99     46119\n",
    "\n",
    "### D. Modelo Gaussian NB\n",
    "\n",
    "\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.36      0.08      0.14      2817\n",
    "           1       0.81      0.96      0.88     11748\n",
    "\n",
    "    accuracy                           0.79     14565\n",
    "   macro avg       0.59      0.52      0.51     14565\n",
    "weighted avg       0.73      0.79      0.74     14565\n",
    "\n",
    "\n",
    "Reusltado de kaagle 0.9932\n",
    "\n",
    "# References\n",
    "\n",
    "Notebooks de la asignatura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
